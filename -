Final Unified Architecture ‚Äì Clinical Document Classification Pipeline
High-Level Architecture Flow (Textual Overview)
	‚Ä¢	Document Ingestion: A PDF document is uploaded to Cloud Storage, triggering the pipeline. The raw PDF is parsed and segmented using a layout-aware extractor (e.g. LayoutParser) to obtain structured text by page. The output is a doc_bundle containing the document ID, page-wise text (with layout metadata), page count, etc.[1]. This ensures the LLM sees content in a structured page context rather than a raw text blob.
	‚Ä¢	Primary Classification (DocumentClassifierAgent): The doc_bundle is fed to the DocumentClassifierAgent, which invokes a Vertex Gemini LLM via the Agent Development Kit (ADK) for inference. Using a comprehensive locked prompt, the LLM analyzes the entire document and produces an initial classification JSON. This classifier_output JSON includes: a list of segments (with start/end pages) each labeled with a dominant document type and any embedded types, a document-level mixture of all five types (each with a presence_level and share of content), the dominant_type_overall, any vendor_signals (e.g. lab names), and a self-evaluation note with reasoning[2]. Crucially, the LLM follows strict rules for multi-label classification, evidence snippet extraction, and internal consistency checks as defined in the prompt[3]. (For example, all five types must appear in the output mixture with at least a NO_EVIDENCE entry, and any type marked present must include page-anchored evidence[4].)
	‚Ä¢	Verification Agents (V1‚ÄìV4): Next, a suite of verifier agents (V1 through V4) independently evaluates the LLM‚Äôs JSON output for specific failure modes[5]. Each verifier focuses on one aspect of quality:
	‚Ä¢	V1 ‚Äì Schema & Completeness Validator: Checks the JSON schema integrity and basic numeric/coverage correctness[6]. It ensures all required keys are present (e.g. each segment has all 5 types listed, the document_mixture covers all 5 types) and that values make sense (shares sum to ~1.0, confidences in range, NO_EVIDENCE types have confidence 0.0, valid page indices, etc.)[6][7]. If minor issues are found (e.g. shares sum to 0.97 due to rounding), V1 can auto-fix them or mark the output for a prompt tweak and retry the classifier for a corrected run[8].
	‚Ä¢	V2 ‚Äì Consistency Checker: Verifies internal consistency of the classification across segments and the document as a whole[9]. It cross-checks that segment-level labels align with the document-level mixture (e.g. if any segment has a type as PRIMARY, the document_mixture for that type must also be PRIMARY)[9]. It recomputes the dominant_type_overall from the segments‚Äô shares and presence weights to ensure the model‚Äôs reported dominant type matches the expected formula and tie-break rules[10]. Any discrepancy indicates the primary LLM may have misapplied the rules, so this agent flags the inconsistency.
	‚Ä¢	V3 ‚Äì Domain Trap Detector: Applies a library of rule-based ‚Äútrap‚Äù checks to catch common misclassification patterns[11]. It looks for known red flags such as a SOAP note trap (clinical note headers like HPI, Assessment/Plan present, but model labeled the segment as Pathology or Genomic)[12], a routine lab trap (presence of lab company names like Quest/LabCorp plus basic lab results, which should indicate an ordinary lab report rather than Genomic)[13], an admin document trap (forms or requisitions mistakenly labeled as clinical reports)[14], or a gene-only trap (Genomic report claimed as PRIMARY based only on gene names without required elements like methodology or accession)[15]. If any of these domain-specific conditions are triggered, V3 flags the segment for likely mislabeling. Severe trap hits (indicating the LLM over-relied on keywords or context was misread) will later inform the arbitration decision.
	‚Ä¢	V4 ‚Äì Evidence Quality Scorer: Assesses the quality of evidence snippets and anchors provided by the LLM[16]. It ensures that for each type labeled (other than NO_EVIDENCE), the cited snippet truly supports the classification. For example, if a segment is labeled Radiology Report, the snippet should contain radiology report cues like a ‚ÄúFindings‚Äù or ‚ÄúImpression‚Äù section ‚Äì not just the word ‚ÄúMRI‚Äù in passing[16]. Likewise, if a Genomic Report is claimed, the agent checks for the presence of genomic-specific anchors (like a gene variant table, ‚ÄúNext-Generation Sequencing‚Äù text, or lab director signature) in the evidence. Weak evidence (e.g. a snippet just mentioning a gene name or a common term without context) will reduce confidence. V4 outputs a score or issues for any snippet that appears to be a superficial or misleading justification[16].
	‚Ä¢	Arbiter & Decision (V5): Finally, the V5 Ambiguity Arbiter agent aggregates all signals from V1‚ÄìV4 (plus the model‚Äôs own self-evaluation notes) to make a routing decision[17]. This agent computes an Ambiguity Score (0‚Äì100) reflecting the level of uncertainty or conflict in the classification[18]. The score is calculated by weighing various factors: e.g. low confidence in the supposed dominant segment, close competition between two possible dominant types, a highly mixed document with no clear primary, any severe traps flagged, or evidence inconsistencies[19]. Based on this score and predefined thresholds, V5 chooses one of three outcomes:
	‚Ä¢	‚úÖ Auto-Accept: If the ambiguity score is low (e.g. <25) and no critical issues were found, the classification is deemed high-confidence and is accepted as final[20]. The pipeline proceeds to finalize the output with minimal intervention.
	‚Ä¢	üîÅ Auto-Retry: If the score is in a mid-range (e.g. 25‚Äì49) or certain fixable errors were identified, the system will attempt an automatic retry[21]. In this case, the pipeline programmatically adjusts the classifier prompt or applies a ‚Äúprompt patch‚Äù (for example, adding a caution or rule reinforcement based on the issues found) and then re-runs the DocumentClassifierAgent on the same input[22]. The new output JSON is then fed back through the V1‚ÄìV4 verifiers for a second pass. (This retry is typically done only once; the pipeline avoids infinite loops by either resolving issues or escalating after one retry.)
	‚Ä¢	üßë‚Äç‚öïÔ∏è SME Escalation: If the ambiguity score is high (e.g. ‚â•50) or the issues are irreconcilable (conflicting signals, low confidence, or severe trap flags), the case is escalated to a human Subject Matter Expert (SME) for manual review[23]. The pipeline does not auto-label this document; instead it prepares a summary of the issues and defers final judgment to an expert.
	‚Ä¢	SME Review & Finalization: For escalated cases, an SMEPacketGenerator (SME Router agent) prepares a comprehensive review packet for the human expert[24]. This includes the original PDF pages (or images) with key sections highlighted, the model‚Äôs current JSON output (segments and labels), the evidence snippets and anchors identified, and an issue report detailing why the system flagged the document (e.g. ambiguity score factors, specific rule violations)[25][26]. The SME uses a dedicated UI to inspect and adjust the classification: they can approve correct segments as-is or edit any mistakes ‚Äì for example, merge/split segments, change a segment‚Äôs dominant type or a type‚Äôs presence_level, adjust the numeric shares (with validations to sum to 1.0), or add missing evidence anchors[27]. The SME‚Äôs corrections are captured as a structured patch (often a JSON diff) describing the changes (e.g. updated labels, notes, and any new rule suggestions the SME has for the team)[28]. After SME review, a Feedback Integrator agent merges the SME‚Äôs patch with the original model output to produce the final validated classification JSON for the document[29]. This final output reflects the SME‚Äôs ground truth adjustments.
	‚Ä¢	Logging & Continuous Learning: Every step produces structured data that is logged for audit and future improvement. The initial model output, the list of issues identified by each verifier, the ambiguity score and decision outcome, and the SME patch (if any) are all saved in a structured store (e.g. BigQuery)[30]. This provides a complete audit trail from raw input to final output[31]. Moreover, the architecture closes the loop by leveraging the accumulated data: recurring SME corrections and common issue patterns feed back into updates for the prompt rules or verifier logic (e.g. adding a new trap rule if SMEs often flag a certain false positive)[32]. The system can thus learn over time, via rapid prompt/rule tweaks or even training lightweight models to predict ambiguity, gradually reducing the need for human intervention[33]. A metrics dashboard (in Looker Studio) tracks key performance indicators ‚Äì for example, the percentage of documents auto-labeled vs. escalated, SME agreement rates (how often the SME changed the model‚Äôs decision), false escalation misses, and overall classification accuracy per document type[34].
(The above text flow can be visualized as a series of stages: Storage & Ingestion ‚ûî LLM Classification ‚ûî Verifier Agents (V1‚ÄìV5) ‚ûî Decision (Accept/Retry/Escalate) ‚ûî SME Review (if needed) ‚ûî Final Output & Logging. Retries loop back once to re-classification, and SME escalation leads to human-in-the-loop correction before finalization.)
Step-by-Step Execution Pipeline
1. PDF Ingestion and Layout Parsing: A new PDF is ingested from Cloud Storage (or an upload event). In the ingestion/preprocessing phase, a parsing service reads the PDF and extracts text while preserving page and layout structure[1]. For each page, text is captured (often using tools like LayoutParser for detecting paragraphs, headers, tables, etc.), and basic metadata like total page count is recorded. The result is a doc_bundle object containing the document‚Äôs text broken into pages (and possibly sections), ready for the classifier[35]. This step filters out any extraneous content (e.g. fax cover sheets, headers/footers) so that only meaningful clinical text is fed forward.
2. Primary Classification (Gemini LLM via ADK): The DocumentClassifierAgent, implemented via Vertex AI‚Äôs Gemini large language model, processes the doc_bundle. Using the Agent Development Kit (ADK), the system provides a carefully crafted prompt (v1.0-D) instructing the LLM how to classify the document. The prompt contains detailed rules and definitions for the five possible document types (‚ÄúClinical Note‚Äù, ‚ÄúRadiology Report‚Äù, ‚ÄúPathology Report‚Äù, ‚ÄúGenomic Report‚Äù, ‚ÄúOther‚Äù) and specifies the required JSON schema/format for output[36][7]. The LLM ‚Äúreasons like a clinician‚Äù through the document, identifying distinct segments and any embedded reports, and assigns each a dominant type[37]. It produces a classification JSON output (classifier_output), strictly adhering to the schema (all expected keys present, no extras)[7]. This JSON includes:
	‚Ä¢	A segments list, where each element represents a contiguous section of the document with its start/end page indices, the dominant_type of that segment, any embedded_types within it, and snippet evidence for each type found.
	‚Ä¢	A document-level document_mixture summary, listing each of the 5 types with a presence_level (PRIMARY, EMBEDDED_RAW, MENTION_ONLY, or NO_EVIDENCE) and an overall_share (percentage of the document) for that type[2].
	‚Ä¢	The computed dominant_type_overall for the entire document (the single best label for routing/analytics, derived from the mixture with rules favoring truly present types over mentions)[38].
	‚Ä¢	Any vendor_signals such as lab or hospital names detected (useful for downstream logic).
	‚Ä¢	A self_evaluation section, where the LLM notes its own confidence and any adjustments it made during reasoning (e.g. a list of changes_made if it downgraded a classification upon self-check)[39].
This primary classification is executed using Vertex AI (hosting the Gemini model) and orchestrated by the ADK which manages the LLM call and tool usage. The result is the initial machine-generated label set for the document.
3. Agent Verifier Layer (V1‚ÄìV4): After the LLM returns its JSON, the pipeline enters the verification layer ‚Äì a series of lightweight agents (or modules) that validate different aspects of the LLM‚Äôs output before we trust it. These V1‚ÄìV4 verifiers run mostly sequentially (some can run in parallel if resources permit) as a quality gate:
	‚Ä¢	V1 ‚Äì Schema & Completeness Check: This verifier performs a deterministic validation of the JSON format and basic data sanity. It ensures that the output JSON exactly matches the expected schema contract[6] ‚Äì all required keys and sections are present (e.g. every segment has a segment_composition entry for each of the 5 types, the document_mixture has all 5 types) and no unknown keys are added[6]. It also checks numeric constraints: all share values should sum to ~1.0 (within a tolerance)[40], confidence scores are within 0.0‚Äì1.0 and follow rules (if a type is NO_EVIDENCE, its confidence must be 0.0)[41], and indices like page numbers fall in valid ranges. If V1 finds minor violations (for instance, a rounding issue or a missing "Other": NO_EVIDENCE entry), it can automatically correct them or mark them for correction. In cases of simple fixes, V1 may adjust the JSON (auto-repair) or append an instruction for the LLM to fix format issues on a retry[42]. More serious schema failures (e.g. significantly malformed JSON) would cause the pipeline to either re-run the classifier with a stricter prompt or escalate if it cannot be resolved.
	‚Ä¢	V2 ‚Äì Cross-Consistency Audit: The second verifier double-checks the logical consistency of the classification across segments and document-level aggregates[9]. It re-applies the core rules from the prompt in a systematic way: for each document type, it ensures the roll-up presence in the document_mixture is consistent with the segment details (e.g. if any segment has a type as PRIMARY, then the document‚Äôs mixture for that type cannot be MENTION_ONLY or NO_EVIDENCE)[9]. It also recalculates the dominant type from scratch: using the shares and presence levels of each type, it computes a dominance score for each (incorporating the hierarchy that PRIMARY > EMBEDDED_RAW > MENTION_ONLY in terms of weight)[10]. The top score determines what the dominant_type_overall should be; V2 checks this against the LLM‚Äôs reported dominant_type_overall. If there‚Äôs a mismatch, it means the LLM made a mistake in applying the dominance rules or a tie-breaker, so an issue is logged. V2 also verifies any self-correction notes from the model; for example, if the model‚Äôs changes_made note says it downgraded a Genomic label due to missing anchors, V2 will ensure that downgrade is reflected correctly in the JSON output.
	‚Ä¢	V3 ‚Äì Rule/Trap Enforcement: V3 is a collection of domain-specific rule checks designed to catch well-known misclassification traps[11]. These are implemented as deterministic checks on both the text and the model‚Äôs output labels:
	‚Ä¢	It looks for clinical note structure (SOAP notes, HPI, Assessment/Plan headers) in any segment that was labeled as a lab or imaging report. If such narrative structures are present, but the model labeled that segment as ‚ÄúPathology‚Äù or ‚ÄúGenomic‚Äù, V3 flags a likely error (the ‚ÄúClinical Note trap‚Äù)[12].
	‚Ä¢	It checks for admin pages or forms (e.g. requisitions, consent forms, or pages containing phrases like ‚ÄúAuthorization Number‚Äù) that the model might have mislabeled as a report. Any segment with those admin cues should usually be ‚ÄúOther‚Äù type; if not, that‚Äôs an ‚ÄúAdmin trap‚Äù trigger[14].
	‚Ä¢	For Genomic and Pathology labels, V3 ensures the presence of critical anchors: e.g. if a segment is labeled Pathology PRIMARY, it should contain things like a gross description, diagnosis section, and pathologist signature. If those are absent, and only a passing reference or some lab terms are present, the model might have overcalled it. Similarly for Genomic PRIMARY, the presence of only gene names without lab report structure is flagged (the ‚Äúgene-only‚Äù trap)[15]. V3 also uses vendor signals: if the text mentions known routine lab companies (Quest, LabCorp) and basic lab test names, but the model chose ‚ÄúGenomic Report‚Äù, that‚Äôs likely a Routine Lab trap ‚Äì V3 would flag that as inconsistent with a true Genomic report[13].
Each trap or rule check that fails produces an issue entry describing the potential misclassification. Some issues can be auto-remedied (for instance, if an admin page was wrongly labeled Genomic, the system could automatically relabel it as Other), whereas others simply raise a red flag for the arbiter. V3‚Äôs output is essentially a list of any rule violations encountered, with severity levels (e.g. ‚Äúwarning‚Äù vs ‚Äúcritical‚Äù) and suggested actions.
	‚Ä¢	V4 ‚Äì Evidence & Quality Check: This verifier evaluates how strong and relevant the evidence is for each labeled type[16]. It reviews the snippets and anchors the LLM provided:
	‚Ä¢	For each segment and type marked present (PRIMARY or EMBEDDED_RAW or MENTION_ONLY), V4 checks if the snippet contains structural indicators of that type. For example, a Radiology segment should have terms like ‚ÄúImpression:‚Äù or imaging findings, not just the word ‚ÄúCT‚Äù embedded in a narrative[43]. A Pathology segment should show evidence of a pathology report section (like ‚ÄúFinal Diagnosis: ‚Ä¶‚Äù). If such context is missing, V4 marks the evidence as weak.
	‚Ä¢	V4 also looks at anchor usage: if the model claimed a high confidence, did it find multiple anchors (e.g. ‚Äúaccession number‚Äù, ‚Äúpathologist signature‚Äù for Pathology, or ‚ÄúMethodology‚Äù and gene list for Genomic)? A high confidence score without the expected anchors is suspicious[44]. Conversely, if only a single keyword triggered a label (e.g. the word ‚Äúmutation‚Äù in a clinical note led to Genomic MENTION_ONLY), V4 might lower the trust in that label.
	‚Ä¢	Additionally, it ensures that vendor names or external identifiers were used appropriately ‚Äì e.g. a mention of ‚ÄúFoundation Medicine‚Äù or ‚ÄúCaris‚Äù (genomic lab companies) should not by itself force a Genomic classification unless accompanied by a real report snippet.
V4 may assign an evidence quality score or simply issue problem flags (like ‚ÄúSnippet for segment 2 ‚Äì Genomic label appears to be just a gene name with no context‚Äù). These results inform the arbiter about how much to trust the model‚Äôs findings beyond just the model‚Äôs raw confidence.
4. Arbiter Decision & Ambiguity Scoring (V5): After the first four verifiers, the pipeline invokes the V5 Ambiguity Arbiter, which is an agent (potentially an LLM-based one) that synthesizes all the findings and decides what to do next[17]. The arbiter takes into account: - The list of issues identified by V1‚ÄìV4 (schema errors, consistency gaps, any trap flags, evidence weaknesses) and the model‚Äôs own self-reported uncertainty signals. - It computes an Ambiguity Score (a numeric score, e.g. 0 to 100) representing the overall confidence in the classification[19]. For instance, the score might start at 0 (certain) and then add points for each sign of uncertainty: if the top two document types‚Äô dominance scores are very close, add points[45]; if the highest confidence segment is still below a threshold (say 70%), add points; if a severe trap (like the admin trap) was triggered, add a significant amount (since that likely indicates a misclassification)[46]; if evidence was judged weak or the model indicated a lot of self-corrections, add more points[47]. The final score is essentially a weighted sum of these factors. - Based on this Ambiguity Score, V5 chooses one of three outcomes: 1. Accept (Auto-label): If the score is low (meaning the classification seems sound) and no showstopper issues were found, V5 will decide to accept the model‚Äôs output as the final result[20]. The pipeline thus proceeds to log the results and skip human review. (Only minor issues might be noted or auto-fixed, and those get logged for transparency.) 2. Retry (Auto-correct): If the score is moderate or there are fixable errors, V5 can choose to auto-retry. In this case, the pipeline formulates a plan to address the errors ‚Äì e.g. it might modify the prompt or give a specific instruction to the LLM to avoid a certain trap or to correct an inconsistency ‚Äì and then call the DocumentClassifierAgent again on the same input[22]. This second run (with a ‚Äúpatched‚Äù prompt or adjusted parameters) is intended to produce a cleaner output. After reclassification, the verifier agents (V1‚ÄìV4) are run again to ensure the new output passes all checks. The arbiter will then make a final decision (accept or escalate) based on the revised output. The retry path is typically executed at most once to limit latency. 3. Escalate to SME: If the Ambiguity Score is high or certain critical issues remain (e.g. conflicting interpretations the model can‚Äôt resolve, or the model consistently fails a rule that a human needs to judge), V5 will decide to escalate the case to a Subject Matter Expert[23]. Along with this decision, the arbiter often generates a concise summary of why the escalation is needed and what questions to ask. For example, it might provide a note like: ‚ÄúAmbiguity: The document has a section that looks like an embedded lab report inside a note. Question for SME ‚Äì Is segment 3 a true lab EMBEDDED_RAW section or just a summarized mention?‚Äù[23]. These targeted questions help focus the SME‚Äôs review on the uncertain parts.
	‚Ä¢	The output of this stage is a small decision.json (or internal object) that includes the computed ambiguity_score, the chosen decision (accept, retry, or escalate), and if escalating, a list of formulated SME review questions or points of confusion to address[48]. This guides the next steps.
5. SME Packet Generation & Review (Human-in-the-Loop): If escalation was chosen, the pipeline transitions into a human-in-the-loop mode: - The SMEPacketGenerator agent (also called the SME Router) prepares a review packet for the SME[24]. This packet typically includes: - A link to the original PDF and/or a viewer with page thumbnails or images. - The model‚Äôs current classification output (segments with their labels and evidence) presented in a readable format, e.g. a table of segments with their assigned types. - All relevant evidence snippets highlighted or listed, along with the anchors_found for each type (so the SME can see why the model thought a certain type was present)[25]. - An issue summary or ‚Äúdisagreement report‚Äù listing what the V-agents found problematic (e.g. ‚ÄúSegment 2: Genomic Report flagged because no accession number found; model unsure‚Äù or ‚ÄúConfusion between Radiology vs Clinical Note in segment 4‚Äù)[49]. - Specific questions for the SME derived from V5‚Äôs analysis, highlighting decisions that need expert input[50]. - The SME opens this packet in a specialized SME Review UI. The UI allows the expert to quickly approve or adjust each part of the model‚Äôs output[51]. For instance, the SME can confirm a segment‚Äôs classification or correct it: change the dominant type of a segment, relabel an embedded report correctly, split or merge segments if the boundaries were wrong, adjust the presence_level of a document type, or tweak the share percentages via sliders (with the UI ensuring the shares re-normalize to 100%)[52][27]. They can also add missing information like a needed anchor snippet, or leave a note explaining a decision. - Once satisfied, the SME submits their changes, which are captured as a structured SME patch (often a JSON document). The sme_patch.json contains the list of modifications: e.g. an array of segment_patches (each indicating a segment and what was changed), any document_mixture_patches (if they adjusted the overall mixture or shares), perhaps a notes section with comments, and even new_rule_suggestions if the SME identified a new scenario that should be addressed by the model in the future[28]. - The pipeline‚Äôs Feedback Integrator then programmatically merges this patch with the original classification JSON to produce the final output for the document[29]. Essentially, the SME‚Äôs labels override the model‚Äôs where changes were made, and the JSON is now considered ground truth for that document. This final label JSON (often stored as final_label.json) represents the validated classification.
Importantly, even when SME intervention occurs, all the issues and changes are logged. The system can learn from this example: for instance, if the SME marked that an embedded report was misclassified as primary, the team might update the LLM prompt rules or V3‚Äôs trap list to catch that pattern next time[32]. This human feedback loop ensures continuous improvement.
6. Logging, Metrics, and Output Storage: Throughout the pipeline, detailed logs and metrics are recorded in BigQuery (or an equivalent analytical database) for monitoring and future analysis[30]. Key data artifacts stored include: - The initial classification output from the LLM (classifier_output JSON). - The list of issues detected by each verifier (each issue with a standardized code, description, severity, and the agent that flagged it)[53]. - The ambiguity score and final decision (accept/retry/escalate) made by the arbiter[54]. - If a retry was done, notes about the prompt modifications and the second-pass results. - If escalated, the SME task record (document ID, who handled it, timestamps) and the SME patch JSON with the corrections[55]. - The final_label JSON (post-SME, if applicable) which is the definitive labeled output for the document[55]. - Any pattern or rule changes applied (e.g. if a prompt rule was updated as a result of this case).
All this information is linked via IDs (run_id, doc_id, task_id) for traceability[56][57]. By querying these logs, one can trace how a given document moved through the pipeline, what issues were found, and how they were resolved ‚Äì providing a complete audit trail[31].
The data in BigQuery is also used to compute performance metrics over time. A Looker Studio dashboard or similar BI tool is connected to these tables to track metrics such as: the proportion of documents auto-accepted vs escalated, the SME escalation rate, the SME ‚Äúagree rate‚Äù (cases where SME didn‚Äôt have to change the model‚Äôs output), average turnaround time for SME reviews, distribution of dominant document types, and frequency of each issue type encountered[34]. These metrics help in managing operations (e.g. ensuring SME workload is sustainable) and in measuring improvements to the model/agents. The pipeline‚Äôs design emphasizes monitoring so that threshold tuning (for ambiguity) and policy updates can be informed by real data[58][59].
Finally, the final outputs ‚Äì the validated classification JSON for each document ‚Äì can be exported or used to update downstream systems as needed (e.g. for indexing documents by type or feeding into an analytics pipeline). Each final label is effectively ‚Äúreviewed‚Äù either by automation or by an SME, ensuring high confidence in the output.
GCP Services and Stack Mapping
Each stage of the architecture is mapped to specific Google Cloud Platform (GCP) services for a scalable, production-ready deployment:
	‚Ä¢	Cloud Storage (PDF Input & Triggers): Incoming PDFs are stored in a Cloud Storage bucket. A new file upload can trigger a Cloud Function or EventArc event to initiate the classification pipeline. Cloud Storage acts as the durable source for document files and provides notifications to kick off processing.
	‚Ä¢	Vertex AI (Gemini LLM for Inference): The core classification and some verification steps use large language model inference. Vertex AI hosts Google‚Äôs Gemini models which the DocumentClassifierAgent calls via the Vertex AI API. The LLM processes text and returns the classification JSON. Likewise, if the Evidence Scorer (V4) or Ambiguity Arbiter (V5) are implemented with LLM reasoning, those are also executed through Vertex AI‚Äôs managed models. This ensures scalable, secure access to the latest LLM (with enterprise features like monitoring and versioning). The Agent Development Kit (ADK) is used to orchestrate these calls within a multi-agent workflow, taking advantage of Vertex‚Äôs integration. ADK is an open-source framework optimized for Gemini and Vertex AI, allowing multi-agent systems to be built with controlled reasoning steps and deployed reliably on GCP‚Äôs infrastructure[60].
	‚Ä¢	ADK Orchestration & Tools: The pipeline‚Äôs agent logic (the coordinator that runs DocumentClassifierAgent then V1‚Ä¶V5, etc.) is built using the Vertex AI Agent Development Kit (ADK). ADK provides the scaffolding for defining each agent‚Äôs behavior, the order of execution, and any tools or functions the agents use (such as layout parsing or schema validation). The multi-agent coordination ‚Äì including passing the document text to the LLM, running Python code for deterministic checks, and handling the control flow for retries or escalation ‚Äì is implemented in the ADK framework. This code can run in a managed Vertex Agent Engine environment or within a custom container. ADK supports deploying agents to Cloud Run or Kubernetes, which means the entire pipeline can be containerized and auto-scaled on GCP infrastructure[61][60]. For example, one might package the pipeline as a Cloud Run service: when a document arrives, a Cloud Run instance spins up to execute all the steps (using Vertex AI for the LLM calls and performing local checks for V1‚ÄìV4), then shuts down when done. Alternatively, individual verifier steps could be separate Cloud Functions for simplicity, but using ADK with Cloud Run ensures the multi-step state is kept in one place.
	‚Ä¢	BigQuery (Data Logging & Analytics): All structured outputs and intermediate results are logged into BigQuery datasets. This includes tables for documents, classification runs, issues, decisions, and SME tasks[62][55]. BigQuery‚Äôs scalable storage and querying make it ideal for accumulating this data over thousands of documents and then analyzing it. For instance, a classification_runs table may store each run‚Äôs document ID, timestamp, model version, the raw classification JSON, the ambiguity score, and the final decision (auto/SME)[63]. An issues table (or JSON field) captures all rule violations and checks the agents produced[64]. An sme_tasks table logs each escalation case and includes the SME‚Äôs patch and the final merged label[55]. Storing this data in BigQuery enables easy writing of SQL queries to compute performance metrics or to drill down into specific cases for debugging.
	‚Ä¢	Looker Studio (Dashboards & Monitoring): BigQuery data is connected to Looker Studio to create live dashboards for the system‚Äôs performance. The development team or operations can monitor metrics like: daily document volume, percent auto-labeled vs escalated, average ambiguity scores, top frequent issue types (e.g. how often the ‚Äúgene-only trap‚Äù fires), SME turnaround times, and accuracy metrics against any ground truth sets[34]. These dashboards provide visibility and help in tuning thresholds or identifying areas for model improvement (for example, if a certain trap is causing frequent escalations, the team might refine the prompt or agent logic to handle it automatically). Looker Studio‚Äôs interactive charts and filters allow slicing the data (by document type, by site, by model version, etc.), which is valuable for both engineering and business stakeholders to understand how the classification system is performing in production.
	‚Ä¢	Cloud Functions / Cloud Run (Serverless Execution): The glue of the pipeline can be implemented using Cloud Functions for event-driven pieces and Cloud Run for the core processing logic. For instance, a Cloud Function could be triggered on PDF upload to invoke a Cloud Run service that runs the ADK multi-agent pipeline. Cloud Run provides a secure, container-based environment to execute the pipeline code with the necessary dependencies (ADK, layout parser, etc.) and can scale out if multiple documents come in concurrently. Each agent (V1‚ÄìV5) might correspond to a function or method within the orchestrator; heavy tasks like LLM calls are done via Vertex AI, while lightweight checks are done in-code. The system could also use Cloud Pub/Sub or Workflows for orchestration, but ADK with Cloud Run keeps the logic self-contained. Cloud Scheduler might periodically trigger evaluation jobs (for offline metrics or reprocessing) and Artifact Registry would store container images for the pipeline. Overall, GCP‚Äôs managed services (Storage, Vertex, BigQuery, Cloud Run) minimize infrastructure work and allow focusing on the classification logic.
(Optionally, if using Vertex AI Agents fully, one could leverage the managed Agent runtime with ADK, but the current design likely uses a mix of custom orchestration and Vertex endpoints. The chosen stack ensures each component (parsing, ML inference, validation, logging) is scalable and maintainable.)
Agent Prompts and Output Schemas
Agent Roles and Prompt Summaries
	‚Ä¢	DocumentClassifierAgent (Primary Classifier): This is the main LLM agent using the v1.0-D locked prompt. Its prompt is extensive and instructs the model to output a JSON classification of the document. The prompt defines all required output fields and rules, including the exact document types and presence level definitions, the need to include evidence snippets for each identified type (with page numbers and anchor text), and the logic for computing overall shares and the dominant type[36][65]. It also contains explicit instructions to perform a ‚Äúself-check‚Äù ‚Äì the model must verify its draft output against the rules and adjust if needed before finalizing[3]. For example, the prompt warns about common pitfalls (like the SOAP note vs. report distinction, and the gene-only trap) and requires the model to populate a changes_made list if it had to correct itself. In summary, the DocumentClassifierAgent‚Äôs prompt guides the LLM through: segmenting the document, assigning each segment a dominant type (and any embedded types), giving every type a presence_level (PRIMARY/EMBEDDED_RAW/MENTION_ONLY/NO_EVIDENCE) for both segments and the whole document, calculating confidence scores based on structural criteria, and providing supporting evidence text for each classification[38][66]. The output is a single JSON adhering to the predefined classifier_output schema.
	‚Ä¢	Verifier V1 (Schema Validator): V1 may not use an LLM at all ‚Äì it‚Äôs typically a deterministic agent (implemented as code or a JSON schema check). Its ‚Äúprompt‚Äù, in concept, is a checklist of schema requirements: ensure no keys are missing or extra, check that each numeric field meets constraints, etc. If implemented as an agent in ADK, it could have a prompt like ‚ÄúVerify the following JSON against the schema and list any discrepancies,‚Äù but more often it‚Äôs just procedural code. V1‚Äôs output is an issues list for any schema or completeness violations (e.g. ‚ÄúMissing Other type in document_mixture‚Äù or ‚ÄúShares sum to 0.97, not 1.0‚Äù) and possibly auto-corrections for trivial fixes[6].
	‚Ä¢	Verifier V2 (Consistency Checker): V2 is also largely deterministic/heuristic. It doesn‚Äôt require an LLM prompt; instead it re-calculates and cross-verifies the model‚Äôs output. In ADK, V2 could be an agent that receives the classifier JSON and runs embedded logic to recompute things like presence consistency and dominant type. If an LLM were used, the prompt might say: ‚ÄúCheck if the segment labels are consistent with the document summary and dominant type. Report any inconsistencies.‚Äù But in practice, this is straightforward enough to do with code. V2 produces issues for any rule breaches, such as ‚ÄúInconsistent presence: segment 3 is Pathology PRIMARY but document_mixture Pathology is MENTION_ONLY‚Äù or ‚ÄúDominant type overall should be Radiology based on shares, but JSON says Genomic.‚Äù[9]
	‚Ä¢	Verifier V3 (Domain Trap Rules): V3 encodes a set of domain-specific rules and thus might be implemented as a combination of regex/keyphrase detectors and logic. As an agent, its ‚Äúprompt‚Äù could be implicit (code) or an actual LLM request if some judgment is needed. For instance, one might use a prompt like: ‚ÄúGiven the document text and initial classification, identify any of the following known issues: (list of traps). Output any detected issues.‚Äù However, since traps are well-defined, the implementation is likely deterministic. V3‚Äôs knowledge base (its ‚Äúprompt‚Äù) includes the definitions of each trap: e.g. Clinical Note Trap: presence of SOAP note sections in what was labeled as a report[67]; Gene-Only Trap: gene names present without report structure[68]; Admin Trap: administrative form indicators in a non-Other segment[69]; etc. Its output is a set of issue objects for each triggered rule, including details like segment/page where it occurred and a recommended fix (e.g. ‚ÄúIf gene-only trap triggered, suggest downgrading Genomic to MENTION_ONLY‚Äù)[53].
	‚Ä¢	Verifier V4 (Evidence Quality Agent): V4 can be implemented either as code or as a secondary LLM call that evaluates snippet quality. If using an LLM, its prompt might be: ‚ÄúReview the following evidence snippet and anchors for the type X. Does this snippet strongly support that the segment is a true X report, or is it possibly a misleading/weak evidence? Explain or score the quality.‚Äù The agent could then output a rating or a verdict (e.g. ‚ÄúSnippet likely just a mention, not a full report, because ...‚Äù). Alternatively, deterministic rules might check anchor keywords. The prompt summary for an LLM-based approach would include guidelines like: ‚ÄúStructural cues to look for: Findings/Impression for Radiology, Final Diagnosis for Pathology, etc. If those are missing and only generic terms are present, flag it.‚Äù[16]. V4 ultimately produces either an adjusted confidence or an issue like ‚ÄúEvidence for type X may be insufficient (only keyword hits without context)‚Äù. These feed into the arbiter‚Äôs decision.
	‚Ä¢	Verifier V5 (Ambiguity Arbiter): V5 might use an LLM to combine all prior signals into a decision. Its prompt (if an LLM is used for the arbiter logic) could be something along the lines of:
‚ÄúYou are a decision-making agent. You will be given: the original classification JSON, and a list of issues found (schema errors, consistency issues, trap flags, evidence concerns), plus the model‚Äôs self-evaluation. Determine an Ambiguity Score from 0 to 100 based on how severe these issues are and how uncertain the classification appears. Then decide one of three actions: ACCEPT if the output is high confidence and issues are minor, RETRY if some issues can be fixed by another model run, or ESCALATE if a human should review. If escalating, provide a brief question or summary for the SME focusing on the uncertainty.‚Äù
The arbiter‚Äôs prompt would encode the thresholds (e.g. if score ‚â•50 then escalate) and possibly examples of each scenario. In implementation, this could also be done with code rules (calculating the score numerically) and then a templated decision. The output of V5 is the decision.json containing the numerical score and the decision label, plus any SME question text if needed[48]. For example: { "ambiguity_score": 72, "decision": "Escalate", "SME_questions": ["Confirm whether segment 2 is an embedded pathology report or just a note reference."] }.
	‚Ä¢	SMEPacketGenerator (SME Router Agent): This is not an LLM agent but a system component that kicks in for escalations. It doesn‚Äôt have a prompt per se; instead, it programmatically gathers data and formats the SME review package[24]. One could consider it an ‚Äúagent‚Äù that prepares the task for SME by collating: the PDF pages (perhaps rendered to images or a link), the current classification and evidence highlights, and a summary of issues/questions. If we were to describe its logic, it might be: ‚ÄúCompile an SME packet: include document reference, model output, and a clear list of points needing human review.‚Äù The agent ensures the SME sees exactly what‚Äôs needed to make a decision quickly (e.g. highlighting the ambiguous segment and giving options). There isn‚Äôt a prompt string, but there is a defined schema for the SME task it creates (with fields like task_id, doc_id, a list of questions, etc.). After the SME completes the task, this agent or another (the Feedback Integrator) will apply the results.
	‚Ä¢	Feedback Integrator Agent: (Not explicitly asked, but for completeness) This component takes the SME‚Äôs patch and the original JSON and merges them. It may not use an LLM; it‚Äôs effectively a transactional update on the data. Its ‚Äúprompt‚Äù is simply to ensure the final output is consistent and then save it. It also might log any new rule suggestions from the SME into a knowledge base for future prompt updates. This agent updates the final records and triggers any follow-up, like updating dashboards or sending a notification that the document is finalized[70].
Each agent above has a clearly defined responsibility and a corresponding prompt or rule set guiding it, which makes the overall system easier to maintain and extend[71]. If a new rule is introduced (say a new trap for a false positive pattern), it would mainly affect the specific verifier agent responsible, without requiring a change to the entire LLM prompt, thereby keeping the architecture modular.
Key Output Schemas and Data Artifacts
	‚Ä¢	Classifier Output JSON (classifier_output): This is the primary output from the DocumentClassifierAgent (LLM). It strictly follows a predefined schema contract[7]. Major fields in this JSON include:
	‚Ä¢	segments ‚Äì an array where each entry has:
	‚Ä¢	segment_id or index, start_page and end_page (defining the segment‚Äôs page range),
	‚Ä¢	dominant_type (one of the five types, the main classification for that segment),
	‚Ä¢	embedded_types (list of any other report types embedded in that segment, if any, typically with their own presence_level),
	‚Ä¢	segment_composition ‚Äì an object giving the presence_level and confidence for each of the 5 types within that segment (e.g. it might show Pathology: PRIMARY (0.85 confidence), Genomic: EMBEDDED_RAW (0.7), others: NO_EVIDENCE (0.0)),
	‚Ä¢	evidence ‚Äì usually a list of snippets for each type found in the segment, with fields like page, snippet text, and anchors_found (anchor keywords indicating that type)[4].
	‚Ä¢	document_mixture ‚Äì an object (or list) summarizing the whole document. It has an entry for each of the 5 types, each entry containing:
	‚Ä¢	presence_level (the overall presence of that type in the document: usually the highest level found among segments, or NO_EVIDENCE if none found),
	‚Ä¢	overall_share (a numeric fraction or percent of the document that this type constitutes, summing to 1.0 across all types)[2],
	‚Ä¢	confidence (the model‚Äôs confidence in the presence classification, often aligned with structural integrity as per the rubric[44]).
	‚Ä¢	dominant_type_overall ‚Äì a single string field giving the model‚Äôs chosen overall document type (the ‚Äúwinner‚Äù for routing). This is determined by a combination of presence level priority and share; it should be one of the types that had PRIMARY or EMBEDDED presence unless everything was just mentions[38].
	‚Ä¢	vendor_signals ‚Äì a list of any vendor or lab names detected (e.g. ‚ÄúLabCorp‚Äù or ‚ÄúMass General Hospital‚Äù) which might be used for downstream analysis.
	‚Ä¢	self_evaluation ‚Äì an object where the model reflects on its output. It often contains:
	‚Ä¢	summary or reasoning text explaining how it decided,
	‚Ä¢	changes_made ‚Äì a list of adjustments it made during the process (e.g. ‚ÄúDowngraded Genomic to MENTION_ONLY after not finding accession number‚Äù)[39].
This classifier_output JSON is the input for the verifiers and is also stored for record. It‚Äôs designed to be comprehensive, so that if it passes all checks, it can directly be used as the final label for the document.
	‚Ä¢	Issues List (issues[]): Throughout verification, any rule violation or anomaly is recorded as an issue object. In the final system, these might be aggregated into an agent_issue_summary_json or similar structure[57]. Each issue typically contains:
	‚Ä¢	An issue_code or IG reference ‚Äì e.g. ‚ÄúSCHEMA_MISSING_KEY‚Äù or ‚ÄúTRAP_GENE_ONLY‚Äù ‚Äì to categorize the type of issue.
	‚Ä¢	A description/message ‚Äì human-readable explanation, like ‚ÄúDocument mixture is missing the ‚ÄòOther‚Äô type entry‚Äù or ‚ÄúGenomic Report mentioned but no sequencing details present (gene-only trap).‚Äù
	‚Ä¢	The severity (e.g. info, warning, critical) indicating how serious the issue is.
	‚Ä¢	The location or context ‚Äì if applicable, which segment or page it pertains to[53].
	‚Ä¢	A recommended action ‚Äì e.g. ‚Äúauto_fix‚Äù for minor ones (the system fixed it or can fix it), ‚Äúneeds_retry‚Äù if a re-run might solve it, or ‚Äúneeds_SME‚Äù if it likely requires human judgment[72].
These issues are produced by V1‚ÄìV4 and possibly V5 (V5 might summarize which issues led to escalation). The issues[] list is invaluable for auditing and for guiding SME: if the case is escalated, this list is presented to the SME to pinpoint what went wrong[73]. Even for accepted cases, storing the issues (which might be empty or only minor auto-fixed ones) allows analysis of common errors and false positives later[74]. In the data pipeline, each run‚Äôs issues list is stored (often as a JSON array in BigQuery for easy querying of frequency of each issue type).
	‚Ä¢	Decision Object (decision.json): This is the output of the V5 arbiter. It‚Äôs a compact JSON containing the decision-making summary for the run. Key fields:
	‚Ä¢	ambiguity_score ‚Äì the numeric score V5 assigned (e.g. 0‚Äì100) reflecting confidence vs uncertainty[19].
	‚Ä¢	decision ‚Äì a categorical outcome: typically values ‚ÄúACCEPT‚Äù, ‚ÄúRETRY‚Äù, or ‚ÄúESCALATE‚Äù (or sometimes phrased as ‚Äúauto_accept‚Äù, etc.)[20].
	‚Ä¢	SME_questions (optional) ‚Äì present if decision was ESCALATE. This could be a list of one or more questions or statements meant for the SME, highlighting what needs attention[50]. For example: ["Is the section on pages 5-6 a standalone Pathology report or part of a note?"].
	‚Ä¢	Optionally, the decision object might include a summary of the critical issues that led to the decision, or references to issue IDs. In some implementations, the arbiter might just output the score and label, and the questions are generated as part of the SME packet assembly. But conceptually, decision.json encapsulates why and how the case was routed.
This object is logged and can be used to tally statistics (like how many were auto-accepted vs escalated, and average ambiguity scores).
	‚Ä¢	SME Patch (sme_patch.json): When an SME intervenes, their corrections are captured in a structured diff-like format. The sme_patch.json includes:
	‚Ä¢	segment_patches ‚Äì an array of entries, each indicating a specific segment modification. For example, one entry might be: {segment_id: 3, dominant_type: "Pathology Report", presence_corrections: {"Genomic Report": "MENTION_ONLY"}, notes: "Segment 3 was a summary of a lab result, not a raw report."} illustrating that for segment 3 the SME changed the dominant type and adjusted a presence level.
	‚Ä¢	document_mixture_patches ‚Äì if the SME changes the overall document mixture or dominant type, that is noted here. E.g. they might set the overall dominant_type_overall to "Clinical Note" if the model had it wrong.
	‚Ä¢	evidence_patches ‚Äì if the SME adds or replaces an evidence snippet (maybe the model‚Äôs snippet was not ideal or the SME wants to highlight a different part), this could be captured.
	‚Ä¢	notes ‚Äì general comments from SME, if any (e.g. ‚ÄúThis document had a pathology consult letter embedded, which confused the model.‚Äù).
	‚Ä¢	new_rule_suggestions ‚Äì an optional list where the SME or annotator can suggest new rules or improvements (for example, ‚ÄúAdd rule: if ‚ÄòAuthorization Number‚Äô present, consider document type Other‚Äù). This is more for feedback to the development team, and can be reviewed later[75].
The patch is structured so that it can be programmatically merged with the original classifier_output JSON to produce an updated JSON. It‚Äôs essentially the delta between the model‚Äôs output and the SME‚Äôs corrected output[28]. This patch is stored for record (so we know exactly what the SME changed) and can be applied to get the final_label_json.
	‚Ä¢	Final Label JSON (final_label.json): This is the ultimate output for the document after all validation and potential human correction. If the document was auto-accepted, the final_label JSON is just the classifier‚Äôs original output (perhaps with minor auto-fixes applied). If SME provided a patch, the final_label is the result of merging that patch. In schema, it‚Äôs essentially the same format as classifier_output (segments, document_mixture, etc.), but now it is considered verified ground truth. The system stores this separately, and it may be used as training data for future model improvements or for downstream consumption by other apps (e.g. an indexing system that now trusts this document‚Äôs labels). The final_label JSON in the logs is also useful to compare against the initial classifier_output to see what changes were made by either agents or SME.
In summary, these artifacts ensure every decision and modification in the pipeline is documented. The schema definitions for these JSON structures are typically fixed and versioned ‚Äì for instance, the classifier_output has a versioned schema (v1.0-D) that all outputs must conform to[7], and similarly the issue objects and SME patch have defined schemas to maintain consistency[73]. This explicit structure allows both machines and humans to inspect the pipeline‚Äôs outcome at each stage, facilitating debugging, auditing, and learning from errors. The final unified architecture thus produces not just a label, but a rich explanatory output for each document, backed by both AI and human verification as needed, all integrated on GCP for scalability and maintainability.
Sources:
	‚Ä¢	HLI Internal Architecture Document ‚Äì ‚ÄúComprehensive Agentic Pipeline (v1.0-D+ with Feedback Loop)‚Äù[76][1][2][6][9][11][16][17].
	‚Ä¢	HLI Internal Design Notes ‚Äì Verifier Agents and SME Loop[22][26][29][30][31].
	‚Ä¢	HLI Prompt Excerpt ‚Äì Primary Classifier v1.0-D Rules[7][66][67][69][68].
	‚Ä¢	HLI Architecture Comparison ‚Äì Issue Tracking and Rule Examples[53][73].
	‚Ä¢	Google Cloud Vertex AI Blog ‚Äì ADK for Multi-Agent Systems[60][61].
	‚Ä¢	HLI Agent Design & Data Schema ‚Äì Agent Responsibilities and Data Model[48][24][28][34][57].

[1] [2] [3] [4] [5] [6] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [26] [27] [29] [30] [31] [32] [33] [38] [39] [40] [41] [42] [43] [45] [46] [47] [50] [53] [58] [59] [64] [71] [73] [74] [76] Comparison of Clinical Document Classification Architectures.docx
file://file_00000000984071f8be4505715015bc57
[7] [36] [37] [44] [65] [66] [67] [68] [69] primary_classifier_agent_prompt.txt
file://file_000000000a1471fd94ab5ec6c74c346e
[24] [25] [28] [34] [35] [48] [49] [51] [52] [54] [55] [56] [57] [62] [63] [70] [72] [75] Agentic_arch.docx
file://file_00000000eaf471f88f889f7e64af1964
[60] [61] Build and manage multi-system agents with Vertex AI | Google Cloud Blog
https://cloud.google.com/blog/products/ai-machine-learning/build-and-manage-multi-system-agents-with-vertex-ai
