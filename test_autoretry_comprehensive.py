"""
Comprehensive AUTO_RETRY Test Suite

Tests all scenarios for the AUTO_RETRY mechanism:
1. Clean document (no retry needed)
2. Single fixable MAJOR issue (should retry and fix)
3. Multiple fixable MAJOR issues (should retry)
4. Max retries reached (should escalate)
5. Non-fixable issues (should escalate)
6. Cycle detection (should escalate)
"""

import json
from pathlib import Path
from datetime import datetime
from src.schemas import ClassificationOutput, DocumentBundle, Issue, IssueSeverity, VerificationReport, ArbiterDecision
from src.agents import RetryOrchestrator, V5ArbiterAgent


class AutoRetryTestSuite:
    """Comprehensive test suite for AUTO_RETRY logic"""
    
    def __init__(self):
        self.results = []
        self.orchestrator = RetryOrchestrator()
        
    def log_result(self, scenario, passed, details):
        """Log test result"""
        self.results.append({
            'scenario': scenario,
            'passed': passed,
            'details': details
        })
        
    def create_mock_doc_bundle(self):
        """Create a simple document bundle for testing"""
        return DocumentBundle(
            doc_id="test-doc-001",
            pdf_filename="test.pdf",
            file_path="/tmp/test.pdf",
            total_pages=5,
            processing_timestamp=datetime.now().isoformat(),
            pages=[{"page_number": i+1, "text": f"Page {i+1}"} for i in range(5)]
        )
    
    def create_mock_report(self, issues):
        """Create a mock verification report"""
        blocker = sum(1 for i in issues if i.severity == IssueSeverity.BLOCKER)
        major = sum(1 for i in issues if i.severity == IssueSeverity.MAJOR)
        minor = sum(1 for i in issues if i.severity == IssueSeverity.MINOR)
        
        return VerificationReport(
            issues=issues,
            v1_validation_passed=(blocker == 0),
            v2_consistency_score=1.0 if len(issues) == 0 else 0.5,
            v3_traps_triggered=0,
            v4_evidence_quality_score=1.0,
            has_blocker_issues=(blocker > 0),
            total_issues=len(issues),
            llm_calls_made=0
        )
    
    # ========== TEST SCENARIOS ==========
    
    def test_scenario_1_clean_document(self):
        """Scenario 1: Clean document with no issues"""
        print("\n" + "="*70)
        print("SCENARIO 1: Clean Document (No Retry Needed)")
        print("="*70)
        
        try:
            # Load valid classification
            with open("output/classification_result.json", 'r') as f:
                data = json.load(f)
            classification = ClassificationOutput(**data)
            doc_bundle = self.create_mock_doc_bundle()
            
            # Run verification
            final_classification, report, decision, retry_log = self.orchestrator.verify_with_retry(
                classification, doc_bundle
            )
            
            # Check expectations
            passed = (
                len(retry_log) == 0 and
                decision.decision == "AUTO_ACCEPT" and
                report.total_issues == 0
            )
            
            details = {
                'retry_attempts': len(retry_log),
                'final_decision': decision.decision,
                'total_issues': report.total_issues
            }
            
            print(f"‚úì Retry attempts: {details['retry_attempts']}")
            print(f"‚úì Final decision: {details['final_decision']}")
            print(f"‚úì Total issues: {details['total_issues']}")
            print(f"\nResult: {'‚úÖ PASSED' if passed else '‚ùå FAILED'}")
            
            self.log_result("Clean Document", passed, details)
            return passed
            
        except Exception as e:
            print(f"‚ùå ERROR: {e}")
            self.log_result("Clean Document", False, {'error': str(e)})
            return False
    
    def test_scenario_2_v5_decision_logic(self):
        """Scenario 2: Test V5 decision logic with mock reports"""
        print("\n" + "="*70)
        print("SCENARIO 2: V5 Decision Logic (Mock Issues)")
        print("="*70)
        
        v5 = V5ArbiterAgent()
        
        # Test 2a: 1 fixable MAJOR ‚Üí AUTO_RETRY
        print("\n  2a: Single fixable MAJOR issue")
        issue_fixable = Issue(
            ig_id="IG-TEST",
            issue_id="TEST-001",
            severity=IssueSeverity.MAJOR,
            agent="V2",
            message="Share sum error",
            location={'segment': 1},
            auto_fixable=True,
            suggested_fix="Normalize shares"
        )
        report = self.create_mock_report([issue_fixable])
        decision = v5.decide(report)
        
        passed_2a = decision.decision == "AUTO_RETRY"
        print(f"    Decision: {decision.decision} ({'‚úÖ PASS' if passed_2a else '‚ùå FAIL'})")
        
        # Test 2b: 1 non-fixable MAJOR ‚Üí ESCALATE_TO_SME
        print("\n  2b: Single non-fixable MAJOR issue")
        issue_nonfixable = Issue(
            ig_id="IG-TEST2",
            issue_id="TEST-002",
            severity=IssueSeverity.MAJOR,
            agent="V3",
            message="Pathology trap",
            location={},
            auto_fixable=False,
            suggested_fix=""
        )
        report = self.create_mock_report([issue_nonfixable])
        decision = v5.decide(report)
        
        passed_2b = decision.decision == "ESCALATE_TO_SME"
        print(f"    Decision: {decision.decision} ({'‚úÖ PASS' if passed_2b else '‚ùå FAIL'})")
        
        passed = passed_2a and passed_2b
        self.log_result("V5 Decision Logic", passed, {
            'fixable_major': 'AUTO_RETRY' if passed_2a else 'WRONG',
            'nonfixable_major': 'ESCALATE' if passed_2b else 'WRONG'
        })
        
        print(f"\nResult: {'‚úÖ PASSED' if passed else '‚ùå FAILED'}")
        return passed
    
    def test_scenario_3_share_normalization_fix(self):
        """Scenario 3: Auto-fix engine can normalize shares"""
        print("\n" + "="*70)
        print("SCENARIO 3: Share Normalization Fix")
        print("="*70)
        
        try:
            from src.agents.auto_fix_engine import AutoFixEngine
            
            # Load valid classification and modify shares
            with open("output/classification_result.json", 'r') as f:
                data = json.load(f)
            classification = ClassificationOutput(**data)
            
            # Inject share error (post-validation)
            print("  Original shares sum:", sum(c.segment_share for c in classification.segments[0].segment_composition))
            classification.segments[0].segment_composition[0].segment_share = 0.10
            modified_sum = sum(c.segment_share for c in classification.segments[0].segment_composition)
            print(f"  Modified shares sum: {modified_sum:.4f}")
            
            # Create fixable issue
            issue = Issue(
                ig_id="IG-TEST3",
                issue_id="TEST-003",
                severity=IssueSeverity.MAJOR,
                agent="V2",
                message="Segment 1: share sum error (sum=1.10)",
                location={'segment': 1},
                auto_fixable=True,
                suggested_fix="Normalize segment shares to sum to 1.0"
            )
            
            # Apply fix
            engine = AutoFixEngine()
            fixed_classification, fixes_log = engine.apply_fixes(classification, [issue])
            
            # Check if normalized
            final_sum = sum(c.segment_share for c in fixed_classification.segments[0].segment_composition)
            normalized = abs(final_sum - 1.0) < 0.0001
            
            print(f"  Fixed shares sum: {final_sum:.6f}")
            print(f"  Fixes applied: {len(fixes_log)}")
            
            passed = normalized and len(fixes_log) > 0
            
            self.log_result("Share Normalization", passed, {
                'original_sum': modified_sum,
                'fixed_sum': final_sum,
                'fixes_applied': len(fixes_log)
            })
            
            print(f"\nResult: {'‚úÖ PASSED' if passed else '‚ùå FAILED'}")
            return passed
            
        except Exception as e:
            print(f"‚ùå ERROR: {e}")
            import traceback
            traceback.print_exc()
            self.log_result("Share Normalization", False, {'error': str(e)})
            return False
    
    def test_scenario_4_max_retries(self):
        """Scenario 4: Max retries reached should escalate"""
        print("\n" + "="*70)
        print("SCENARIO 4: Max Retries Limit (Should Escalate)")
        print("="*70)
        
        # This is conceptual - would need to simulate a fix that doesn't resolve issues
        # For now, just verify the logic exists
        print("  Max retries constant:", self.orchestrator.MAX_RETRIES)
        print("  Note: Full test requires simulation of persistent issues")
        
        passed = self.orchestrator.MAX_RETRIES == 2
        self.log_result("Max Retries Limit", passed, {'max_retries': self.orchestrator.MAX_RETRIES})
        
        print(f"\nResult: {'‚úÖ PASSED' if passed else '‚ùå FAILED'} (Logic verified)")
        return passed
    
    def test_scenario_5_cycle_detection(self):
        """Scenario 5: Cycle detection should prevent infinite loops"""
        print("\n" + "="*70)
        print("SCENARIO 5: Cycle Detection (Fingerprinting)")
        print("="*70)
        
        try:
            # Load classification
            with open("output/classification_result.json", 'r') as f:
                data = json.load(f)
            classification = ClassificationOutput(**data)
            
            # Test fingerprint consistency
            fingerprint1 = self.orchestrator._get_classification_fingerprint(classification)
            fingerprint2 = self.orchestrator._get_classification_fingerprint(classification)
            
            # Modify classification slightly
            classification.segments[0].segment_composition[0].segment_share = 0.50
            fingerprint3 = self.orchestrator._get_classification_fingerprint(classification)
            
            same_when_unchanged = (fingerprint1 == fingerprint2)
            different_when_changed = (fingerprint1 != fingerprint3)
            
            print(f"  Fingerprint unchanged: {fingerprint1 == fingerprint2} ({'‚úÖ' if same_when_unchanged else '‚ùå'})")
            print(f"  Fingerprint changes: {fingerprint1 != fingerprint3} ({'‚úÖ' if different_when_changed else '‚ùå'})")
            
            passed = same_when_unchanged and different_when_changed
            self.log_result("Cycle Detection", passed, {
                'consistent_fingerprints': same_when_unchanged,
                'detects_changes': different_when_changed
            })
            
            print(f"\nResult: {'‚úÖ PASSED' if passed else '‚ùå FAILED'}")
            return passed
            
        except Exception as e:
            print(f"‚ùå ERROR: {e}")
            self.log_result("Cycle Detection", False, {'error': str(e)})
            return False
    
    def run_all_tests(self):
        """Run all test scenarios"""
        print("\n" + "="*70)
        print("AUTO_RETRY COMPREHENSIVE TEST SUITE")
        print("="*70)
        print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Run all scenarios
        results = []
        results.append(self.test_scenario_1_clean_document())
        results.append(self.test_scenario_2_v5_decision_logic())
        results.append(self.test_scenario_3_share_normalization_fix())
        results.append(self.test_scenario_4_max_retries())
        results.append(self.test_scenario_5_cycle_detection())
        
        # Summary
        print("\n" + "="*70)
        print("TEST SUMMARY")
        print("="*70)
        
        for i, result in enumerate(self.results, 1):
            status = "‚úÖ PASS" if result['passed'] else "‚ùå FAIL"
            print(f"{i}. {result['scenario']}: {status}")
            if not result['passed'] and 'error' in result['details']:
                print(f"   Error: {result['details']['error']}")
        
        total = len(self.results)
        passed = sum(1 for r in self.results if r['passed'])
        
        print(f"\n{'='*70}")
        print(f"TOTAL: {passed}/{total} tests passed ({passed/total*100:.0f}%)")
        print(f"{'='*70}")
        
        return all(results)


def main():
    """Run the test suite and generate report"""
    suite = AutoRetryTestSuite()
    success = suite.run_all_tests()
    
    # Save report to file
    report_path = Path("output/autoretry_test_report.json")
    report_data = {
        'timestamp': datetime.now().isoformat(),
        'total_tests': len(suite.results),
        'passed': sum(1 for r in suite.results if r['passed']),
        'failed': sum(1 for r in suite.results if not r['passed']),
        'results': suite.results
    }
    
    with open(report_path, 'w') as f:
        json.dump(report_data, f, indent=2)
    
    print(f"\nüìÑ Detailed report saved to: {report_path}")
    
    return 0 if success else 1


if __name__ == "__main__":
    import sys
    try:
        sys.exit(main())
    except Exception as e:
        print(f"\n‚ùå FATAL ERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
